{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-09T01:45:41.236882Z",
     "start_time": "2025-05-09T01:45:41.176385Z"
    }
   },
   "source": [
    "import time\n",
    "from sklearn.datasets import load_iris, fetch_20newsgroups, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "load直接加载的内存的，数据集比较小，并不会保存到本地磁盘\n",
    "fetch数据集比较大，下载下来后会存在本地磁盘，下一次就不会再连接sklearn的服务器\n"
   ],
   "id": "5b86605fa711033c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:16:13.778269Z",
     "start_time": "2025-05-07T13:16:13.763780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "li = load_iris()\n",
    "print(\"获取特征值\")\n",
    "print(type(li.data))\n",
    "print('-' * 50)\n",
    "print(li.data.shape)\n",
    "li.data"
   ],
   "id": "ec9a2c5c0d89a319",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取特征值\n",
      "<class 'numpy.ndarray'>\n",
      "--------------------------------------------------\n",
      "(150, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:19:18.629707Z",
     "start_time": "2025-05-07T13:19:18.624366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"目标值\")\n",
    "print(li.target)\n",
    "print('-' * 50)\n",
    "print(li.DESCR)\n",
    "print('-' * 50)\n",
    "print(li.feature_names)"
   ],
   "id": "8ba382b1fca12bcb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目标值\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "--------------------------------------------------\n",
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "    Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "    Structure and Classification Rule for Recognition in Partially Exposed\n",
      "    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "    on Information Theory, May 1972, 431-433.\n",
      "  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "    conceptual clustering system finds 3 classes in the data.\n",
      "  - Many, many more ...\n",
      "\n",
      "--------------------------------------------------\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:24:19.167326Z",
     "start_time": "2025-05-07T13:24:19.152313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 注意返回值, 训练集 train  x_train, y_train        测试集  test   x_test, y_test，顺序千万别搞错了\n",
    "# 默认是乱序的,random_state为了确保两次的随机策略一致，就会得到相同的随机数据，往往会带上\n",
    "x_train, x_test, y_train, y_test = train_test_split(li.data, li.target, test_size=0.25, random_state=1)\n",
    "\n",
    "print(\"训练集特征值和目标值：\", x_train, y_train)\n",
    "print(\"训练集特征值shape\", x_train.shape)\n",
    "print('-'*50)\n",
    "print(\"测试集特征值和目标值：\", x_test, y_test)\n",
    "print(\"测试集特征值shape\", x_test.shape)"
   ],
   "id": "17c409f1ca36fb1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集特征值和目标值： [[6.5 2.8 4.6 1.5]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [4.9 3.6 1.4 0.1]] [1 2 2 0 2 2 1 2 0 0 0 1 0 0 2 2 2 2 2 1 2 1 0 2 2 0 0 2 0 2 2 1 1 2 2 0 1\n",
      " 1 2 1 2 1 0 0 0 2 0 1 2 2 0 0 1 0 2 1 2 2 1 2 2 1 0 1 0 1 1 0 1 0 0 2 2 2\n",
      " 0 0 1 0 2 0 2 2 0 2 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 2 0 0 2 1 2 1 2 2 1 2\n",
      " 0]\n",
      "训练集特征值shape (112, 4)\n",
      "--------------------------------------------------\n",
      "测试集特征值和目标值： [[5.8 4.  1.2 0.2]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.7 3.  5.  1.7]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.2 3.4 1.4 0.2]] [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n",
      " 0]\n",
      "测试集特征值shape (38, 4)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:29:28.750586Z",
     "start_time": "2025-05-07T13:29:28.500941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 下面是比较大的数据，需要下载一会，20类新闻\n",
    "#subset代表下载的数据集类型，默认是train，只有训练集\n",
    "news = fetch_20newsgroups(subset='all', data_home='../python_ml/data')\n",
    "# print(news.feature_names)  #这个数据集是没有的，因为没有特征，只有文本数据\n",
    "# print(news.DESCR)\n",
    "print('第一个样本')\n",
    "print(news.data[0])\n",
    "print('特征类型')\n",
    "print(type(news.data))\n",
    "print('-' * 50)\n",
    "print(news.target[0:15])\n",
    "from pprint import pprint\n",
    "pprint(list(news.target_names))"
   ],
   "id": "94b6c44c588105e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个样本\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "特征类型\n",
      "<class 'list'>\n",
      "--------------------------------------------------\n",
      "[10  3 17  3  4 12  4 10 10 19 19 11 19 13  0]\n",
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:32:54.440339Z",
     "start_time": "2025-05-07T13:32:54.436344Z"
    }
   },
   "cell_type": "code",
   "source": "print(news.DESCR)",
   "id": "f2b9cbf8b0e05124",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "=================   ==========\n",
      "Classes                     20\n",
      "Samples total            18846\n",
      "Dimensionality               1\n",
      "Features                  text\n",
      "=================   ==========\n",
      "\n",
      ".. dropdown:: Usage\n",
      "\n",
      "  The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "  fetching / caching functions that downloads the data archive from\n",
      "  the original `20 newsgroups website <http://people.csail.mit.edu/jrennie/20Newsgroups/>`__,\n",
      "  extracts the archive contents\n",
      "  in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      "  :func:`sklearn.datasets.load_files` on either the training or\n",
      "  testing set folder, or both of them::\n",
      "\n",
      "    >>> from sklearn.datasets import fetch_20newsgroups\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "    >>> from pprint import pprint\n",
      "    >>> pprint(list(newsgroups_train.target_names))\n",
      "    ['alt.atheism',\n",
      "     'comp.graphics',\n",
      "     'comp.os.ms-windows.misc',\n",
      "     'comp.sys.ibm.pc.hardware',\n",
      "     'comp.sys.mac.hardware',\n",
      "     'comp.windows.x',\n",
      "     'misc.forsale',\n",
      "     'rec.autos',\n",
      "     'rec.motorcycles',\n",
      "     'rec.sport.baseball',\n",
      "     'rec.sport.hockey',\n",
      "     'sci.crypt',\n",
      "     'sci.electronics',\n",
      "     'sci.med',\n",
      "     'sci.space',\n",
      "     'soc.religion.christian',\n",
      "     'talk.politics.guns',\n",
      "     'talk.politics.mideast',\n",
      "     'talk.politics.misc',\n",
      "     'talk.religion.misc']\n",
      "\n",
      "  The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "  attribute is the integer index of the category::\n",
      "\n",
      "    >>> newsgroups_train.filenames.shape\n",
      "    (11314,)\n",
      "    >>> newsgroups_train.target.shape\n",
      "    (11314,)\n",
      "    >>> newsgroups_train.target[:10]\n",
      "    array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "  It is possible to load only a sub-selection of the categories by passing the\n",
      "  list of the categories to load to the\n",
      "  :func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "    >>> cats = ['alt.atheism', 'sci.space']\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "    >>> list(newsgroups_train.target_names)\n",
      "    ['alt.atheism', 'sci.space']\n",
      "    >>> newsgroups_train.filenames.shape\n",
      "    (1073,)\n",
      "    >>> newsgroups_train.target.shape\n",
      "    (1073,)\n",
      "    >>> newsgroups_train.target[:10]\n",
      "    array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      ".. dropdown:: Converting text to vectors\n",
      "\n",
      "  In order to feed predictive or clustering models with the text data,\n",
      "  one first need to turn the text into vectors of numerical values suitable\n",
      "  for statistical analysis. This can be achieved with the utilities of the\n",
      "  ``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "  example that extract `TF-IDF <https://en.wikipedia.org/wiki/Tf-idf>`__ vectors\n",
      "  of unigram tokens from a subset of 20news::\n",
      "\n",
      "    >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "    >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "    ...               'comp.graphics', 'sci.space']\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "    ...                                       categories=categories)\n",
      "    >>> vectorizer = TfidfVectorizer()\n",
      "    >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "    >>> vectors.shape\n",
      "    (2034, 34118)\n",
      "\n",
      "  The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "  components by sample in a more than 30000-dimensional space\n",
      "  (less than .5% non-zero features)::\n",
      "\n",
      "    >>> vectors.nnz / float(vectors.shape[0])\n",
      "    159.01327...\n",
      "\n",
      "  :func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which\n",
      "  returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. dropdown:: Filtering text for more realistic training\n",
      "\n",
      "  It is easy for a classifier to overfit on particular things that appear in the\n",
      "  20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "  high F-scores, but their results would not generalize to other documents that\n",
      "  aren't from this window of time.\n",
      "\n",
      "  For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "  which is fast to train and achieves a decent F-score::\n",
      "\n",
      "    >>> from sklearn.naive_bayes import MultinomialNB\n",
      "    >>> from sklearn import metrics\n",
      "    >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "    ...                                      categories=categories)\n",
      "    >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "    >>> clf = MultinomialNB(alpha=.01)\n",
      "    >>> clf.fit(vectors, newsgroups_train.target)\n",
      "    MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "    >>> pred = clf.predict(vectors_test)\n",
      "    >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "    0.88213...\n",
      "\n",
      "  (The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "  the training and test data, instead of segmenting by time, and in that case\n",
      "  multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "  yet of what's going on inside this classifier?)\n",
      "\n",
      "  Let's take a look at what the most informative features are:\n",
      "\n",
      "    >>> import numpy as np\n",
      "    >>> def show_top10(classifier, vectorizer, categories):\n",
      "    ...     feature_names = vectorizer.get_feature_names_out()\n",
      "    ...     for i, category in enumerate(categories):\n",
      "    ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "    ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "    ...\n",
      "    >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "    alt.atheism: edu it and in you that is of to the\n",
      "    comp.graphics: edu in graphics it is for and of to the\n",
      "    sci.space: edu it that is in and space to of the\n",
      "    talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "  You can now see many things that these features have overfit to:\n",
      "\n",
      "  - Almost every group is distinguished by whether headers such as\n",
      "    ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "  - Another significant feature involves whether the sender is affiliated with\n",
      "    a university, as indicated either by their headers or their signature.\n",
      "  - The word \"article\" is a significant feature, based on how often people quote\n",
      "    previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "    wrote:\"\n",
      "  - Other features match the names and e-mail addresses of particular people who\n",
      "    were posting at the time.\n",
      "\n",
      "  With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "  barely have to identify topics from text at all, and they all perform at the\n",
      "  same high level.\n",
      "\n",
      "  For this reason, the functions that load 20 Newsgroups data provide a\n",
      "  parameter called **remove**, telling it what kinds of information to strip out\n",
      "  of each file. **remove** should be a tuple containing any subset of\n",
      "  ``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "  blocks, and quotation blocks respectively.\n",
      "\n",
      "    >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "    ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "    ...                                      categories=categories)\n",
      "    >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "    >>> pred = clf.predict(vectors_test)\n",
      "    >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
      "    0.77310...\n",
      "\n",
      "  This classifier lost over a lot of its F-score, just because we removed\n",
      "  metadata that has little to do with topic classification.\n",
      "  It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "    >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "    ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "    ...                                       categories=categories)\n",
      "    >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "    >>> clf = MultinomialNB(alpha=.01)\n",
      "    >>> clf.fit(vectors, newsgroups_train.target)\n",
      "    MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "    >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "    >>> pred = clf.predict(vectors_test)\n",
      "    >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "    0.76995...\n",
      "\n",
      "  Some other classifiers cope better with this harder version of the task. Try the\n",
      "  :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`\n",
      "  example with and without the `remove` option to compare the results.\n",
      "\n",
      ".. rubric:: Data Considerations\n",
      "\n",
      "The Cleveland Indians is a major league baseball team based in Cleveland,\n",
      "Ohio, USA. In December 2020, it was reported that \"After several months of\n",
      "discussion sparked by the death of George Floyd and a national reckoning over\n",
      "race and colonialism, the Cleveland Indians have decided to change their\n",
      "name.\" Team owner Paul Dolan \"did make it clear that the team will not make\n",
      "its informal nickname -- the Tribe -- its new team name.\" \"It's not going to\n",
      "be a half-step away from the Indians,\" Dolan said.\"We will not have a Native\n",
      "American-themed name.\"\n",
      "\n",
      "https://www.mlb.com/news/cleveland-indians-team-name-change\n",
      "\n",
      ".. rubric:: Recommendation\n",
      "\n",
      "- When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "  should strip newsgroup-related metadata. In scikit-learn, you can do this\n",
      "  by setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "  lower because it is more realistic.\n",
      "- This text dataset contains data which may be inappropriate for certain NLP\n",
      "  applications. An example is listed in the \"Data Considerations\" section\n",
      "  above. The challenge with using current text datasets in NLP for tasks such\n",
      "  as sentence completion, clustering, and other applications is that text\n",
      "  that is culturally biased and inflammatory will propagate biases. This\n",
      "  should be taken into consideration when using the dataset, reviewing the\n",
      "  output, and the bias should be documented.\n",
      "\n",
      ".. rubric:: Examples\n",
      "\n",
      "* :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`\n",
      "* :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "* :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`\n",
      "* :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:38:08.479034Z",
     "start_time": "2025-05-07T13:38:08.472788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(news.data))\n",
    "print('-' * 50)\n",
    "print(news.target)\n",
    "print('-' * 50)\n",
    "print(min(news.target),max(news.target))"
   ],
   "id": "b5e0c05ec8b55a96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "--------------------------------------------------\n",
      "[10  3 17 ...  3  1  7]\n",
      "--------------------------------------------------\n",
      "0 19\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:40:43.106122Z",
     "start_time": "2025-05-07T13:40:43.101128Z"
    }
   },
   "cell_type": "code",
   "source": "pprint(news.target_names)",
   "id": "d117e80685cc12e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:42:37.538948Z",
     "start_time": "2025-05-07T13:42:37.524947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "house=fetch_california_housing(data_home='../python_ml/data')\n",
    "print(\"获取特征值\")\n",
    "print(house.data[0])  #第一个样本特征值\n",
    "print('样本的形状')\n",
    "print(house.data.shape)\n",
    "print('-' * 50)\n"
   ],
   "id": "722594e4acbb6623",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取特征值\n",
      "[   8.3252       41.            6.98412698    1.02380952  322.\n",
      "    2.55555556   37.88       -122.23      ]\n",
      "样本的形状\n",
      "(20640, 8)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:43:51.641421Z",
     "start_time": "2025-05-07T13:43:51.636426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"目标值\")\n",
    "print(house.target)\n",
    "print('-' * 50)\n",
    "print(house.DESCR)\n",
    "print('-' * 50)\n",
    "print(house.feature_names)\n",
    "print('-' * 50)"
   ],
   "id": "4d73e52a72227c7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目标值\n",
      "[4.526 3.585 3.521 ... 0.923 0.847 0.894]\n",
      "--------------------------------------------------\n",
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 20640\n",
      "\n",
      ":Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      ":Attribute Information:\n",
      "    - MedInc        median income in block group\n",
      "    - HouseAge      median house age in block group\n",
      "    - AveRooms      average number of rooms per household\n",
      "    - AveBedrms     average number of bedrooms per household\n",
      "    - Population    block group population\n",
      "    - AveOccup      average number of household members\n",
      "    - Latitude      block group latitude\n",
      "    - Longitude     block group longitude\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. rubric:: References\n",
      "\n",
      "- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "  Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n",
      "--------------------------------------------------\n",
      "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2 分类估计器",
   "id": "2865f1bdb9fd7454"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T01:46:54.770781Z",
     "start_time": "2025-05-09T01:46:41.604108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# K近邻\n",
    "\"\"\"\n",
    "K-近邻预测用户签到位置\n",
    ":return:None\n",
    "\"\"\"\n",
    "# 读取数据\n",
    "data = pd.read_csv(\"../python_ml/data/FBlocation/train.csv\")\n",
    "\n",
    "print(data.head(10))\n",
    "print(data.shape)\n",
    "print(data.info())\n",
    "# 处理数据\n",
    "# 1、缩小数据,查询数据,为了减少计算时间\n",
    "data = data.query(\"x > 1.0 &  x < 1.25 & y > 2.5 & y < 2.75\")\n",
    "\n"
   ],
   "id": "42d42f8d0ef0b91d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_id       x       y  accuracy    time    place_id\n",
      "0       0  0.7941  9.0809        54  470702  8523065625\n",
      "1       1  5.9567  4.7968        13  186555  1757726713\n",
      "2       2  8.3078  7.0407        74  322648  1137537235\n",
      "3       3  7.3665  2.5165        65  704587  6567393236\n",
      "4       4  4.0961  1.1307        31  472130  7440663949\n",
      "5       5  3.8099  1.9586        75  178065  6289802927\n",
      "6       6  6.3336  4.3720        13  666829  9931249544\n",
      "7       7  5.7409  6.7697        85  369002  5662813655\n",
      "8       8  4.3114  6.9410         3  166384  8471780938\n",
      "9       9  6.3414  0.0758        65  400060  1253803156\n",
      "(29118021, 6)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29118021 entries, 0 to 29118020\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Dtype  \n",
      "---  ------    -----  \n",
      " 0   row_id    int64  \n",
      " 1   x         float64\n",
      " 2   y         float64\n",
      " 3   accuracy  int64  \n",
      " 4   time      int64  \n",
      " 5   place_id  int64  \n",
      "dtypes: float64(2), int64(4)\n",
      "memory usage: 1.3 GB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T01:49:05.871278Z",
     "start_time": "2025-05-09T01:49:05.866006Z"
    }
   },
   "cell_type": "code",
   "source": "print(data.shape)",
   "id": "760c6f4857db9281",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17710, 6)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T01:49:01.788351Z",
     "start_time": "2025-05-09T01:49:01.767110Z"
    }
   },
   "cell_type": "code",
   "source": "data.describe()",
   "id": "f77887b8fe088fc8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             row_id             x             y      accuracy           time  \\\n",
       "count  1.771000e+04  17710.000000  17710.000000  17710.000000   17710.000000   \n",
       "mean   1.450569e+07      1.122538      2.632309     82.482101  397551.263128   \n",
       "std    8.353805e+06      0.077086      0.070144    113.613227  234601.097883   \n",
       "min    6.000000e+02      1.000100      2.500100      1.000000     119.000000   \n",
       "25%    7.327816e+06      1.049200      2.573800     25.000000  174069.750000   \n",
       "50%    1.443071e+07      1.123300      2.642300     62.000000  403387.500000   \n",
       "75%    2.163463e+07      1.190500      2.687800     75.000000  602111.750000   \n",
       "max    2.911215e+07      1.249900      2.749900   1004.000000  786218.000000   \n",
       "\n",
       "           place_id  \n",
       "count  1.771000e+04  \n",
       "mean   5.129895e+09  \n",
       "std    2.357399e+09  \n",
       "min    1.012024e+09  \n",
       "25%    3.312464e+09  \n",
       "50%    5.261906e+09  \n",
       "75%    6.766325e+09  \n",
       "max    9.980711e+09  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "      <th>place_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.771000e+04</td>\n",
       "      <td>17710.000000</td>\n",
       "      <td>17710.000000</td>\n",
       "      <td>17710.000000</td>\n",
       "      <td>17710.000000</td>\n",
       "      <td>1.771000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.450569e+07</td>\n",
       "      <td>1.122538</td>\n",
       "      <td>2.632309</td>\n",
       "      <td>82.482101</td>\n",
       "      <td>397551.263128</td>\n",
       "      <td>5.129895e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.353805e+06</td>\n",
       "      <td>0.077086</td>\n",
       "      <td>0.070144</td>\n",
       "      <td>113.613227</td>\n",
       "      <td>234601.097883</td>\n",
       "      <td>2.357399e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000e+02</td>\n",
       "      <td>1.000100</td>\n",
       "      <td>2.500100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>1.012024e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.327816e+06</td>\n",
       "      <td>1.049200</td>\n",
       "      <td>2.573800</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>174069.750000</td>\n",
       "      <td>3.312464e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.443071e+07</td>\n",
       "      <td>1.123300</td>\n",
       "      <td>2.642300</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>403387.500000</td>\n",
       "      <td>5.261906e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.163463e+07</td>\n",
       "      <td>1.190500</td>\n",
       "      <td>2.687800</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>602111.750000</td>\n",
       "      <td>6.766325e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.911215e+07</td>\n",
       "      <td>1.249900</td>\n",
       "      <td>2.749900</td>\n",
       "      <td>1004.000000</td>\n",
       "      <td>786218.000000</td>\n",
       "      <td>9.980711e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T01:55:22.842998Z",
     "start_time": "2025-05-09T01:55:22.836634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "time_value = pd.to_datetime(data['time'], unit='s')\n",
    "print(time_value.head(10))"
   ],
   "id": "734dcf90548ee97a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600    1970-01-01 18:09:40\n",
      "957    1970-01-10 02:11:10\n",
      "4345   1970-01-05 15:08:02\n",
      "4735   1970-01-06 23:03:03\n",
      "5580   1970-01-09 11:26:50\n",
      "6090   1970-01-02 16:25:07\n",
      "6234   1970-01-04 15:52:57\n",
      "6350   1970-01-01 10:13:36\n",
      "7468   1970-01-09 15:26:06\n",
      "8478   1970-01-08 23:52:02\n",
      "Name: time, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T01:56:26.883997Z",
     "start_time": "2025-05-09T01:56:26.877410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 把日期格式转换成 字典格式，把年，月，日，时，分，秒转换为字典格式，\n",
    "time_value = pd.DatetimeIndex(time_value)\n",
    "#\n",
    "print('-' * 50)\n",
    "print(time_value[0:10])"
   ],
   "id": "4127030c6d802fd7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "DatetimeIndex(['1970-01-01 18:09:40', '1970-01-10 02:11:10',\n",
      "               '1970-01-05 15:08:02', '1970-01-06 23:03:03',\n",
      "               '1970-01-09 11:26:50', '1970-01-02 16:25:07',\n",
      "               '1970-01-04 15:52:57', '1970-01-01 10:13:36',\n",
      "               '1970-01-09 15:26:06', '1970-01-08 23:52:02'],\n",
      "              dtype='datetime64[ns]', name='time', freq=None)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:00:55.008136Z",
     "start_time": "2025-05-09T02:00:54.622187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(type(data))\n",
    "data.insert(data.shape[1], 'day', time_value.day)\n",
    "data.insert(data.shape[1], 'hour', time_value.hour)\n",
    "data.insert(data.shape[1], 'weekday', time_value.weekday) # 0代表周一，6代表周日\n",
    "data.head()"
   ],
   "id": "60e844944f77aaa6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot insert day, already exists",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[32]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mtype\u001B[39m(data))\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mday\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtime_value\u001B[49m\u001B[43m.\u001B[49m\u001B[43mday\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m data.insert(data.shape[\u001B[32m1\u001B[39m], \u001B[33m'\u001B[39m\u001B[33mhour\u001B[39m\u001B[33m'\u001B[39m, time_value.hour)\n\u001B[32m      4\u001B[39m data.insert(data.shape[\u001B[32m1\u001B[39m], \u001B[33m'\u001B[39m\u001B[33mweekday\u001B[39m\u001B[33m'\u001B[39m, time_value.weekday) \u001B[38;5;66;03m# 0代表周一，6代表周日\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:5158\u001B[39m, in \u001B[36mDataFrame.insert\u001B[39m\u001B[34m(self, loc, column, value, allow_duplicates)\u001B[39m\n\u001B[32m   5152\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   5153\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mCannot specify \u001B[39m\u001B[33m'\u001B[39m\u001B[33mallow_duplicates=True\u001B[39m\u001B[33m'\u001B[39m\u001B[33m when \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   5154\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m'\u001B[39m\u001B[33mself.flags.allows_duplicate_labels\u001B[39m\u001B[33m'\u001B[39m\u001B[33m is False.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   5155\u001B[39m     )\n\u001B[32m   5156\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_duplicates \u001B[38;5;129;01mand\u001B[39;00m column \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.columns:\n\u001B[32m   5157\u001B[39m     \u001B[38;5;66;03m# Should this be a different kind of error??\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m5158\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mcannot insert \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcolumn\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, already exists\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   5159\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_integer(loc):\n\u001B[32m   5160\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mloc must be int\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mValueError\u001B[39m: cannot insert day, already exists"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:01:11.950238Z",
     "start_time": "2025-05-09T02:01:11.944076Z"
    }
   },
   "cell_type": "code",
   "source": "data.drop('time', axis=1, inplace=True)",
   "id": "8567752212f3f17a",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:01:28.637441Z",
     "start_time": "2025-05-09T02:01:28.627462Z"
    }
   },
   "cell_type": "code",
   "source": "data.head(10)",
   "id": "f6669f4fa12c4742",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      row_id       x       y  accuracy    place_id  day  hour  weekday\n",
       "600      600  1.2214  2.7023        17  6683426742    1    18        3\n",
       "957      957  1.1832  2.6891        58  6683426742   10     2        5\n",
       "4345    4345  1.1935  2.6550        11  6889790653    5    15        0\n",
       "4735    4735  1.1452  2.6074        49  6822359752    6    23        1\n",
       "5580    5580  1.0089  2.7287        19  1527921905    9    11        4\n",
       "6090    6090  1.1140  2.6262        11  4000153867    2    16        4\n",
       "6234    6234  1.1449  2.5003        34  3741484405    4    15        6\n",
       "6350    6350  1.0844  2.7436        65  5963693798    1    10        3\n",
       "7468    7468  1.0058  2.5096        66  9076695703    9    15        4\n",
       "8478    8478  1.2015  2.5187        72  3992589015    8    23        3"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>place_id</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>600</td>\n",
       "      <td>1.2214</td>\n",
       "      <td>2.7023</td>\n",
       "      <td>17</td>\n",
       "      <td>6683426742</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>957</td>\n",
       "      <td>1.1832</td>\n",
       "      <td>2.6891</td>\n",
       "      <td>58</td>\n",
       "      <td>6683426742</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4345</th>\n",
       "      <td>4345</td>\n",
       "      <td>1.1935</td>\n",
       "      <td>2.6550</td>\n",
       "      <td>11</td>\n",
       "      <td>6889790653</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4735</th>\n",
       "      <td>4735</td>\n",
       "      <td>1.1452</td>\n",
       "      <td>2.6074</td>\n",
       "      <td>49</td>\n",
       "      <td>6822359752</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5580</th>\n",
       "      <td>5580</td>\n",
       "      <td>1.0089</td>\n",
       "      <td>2.7287</td>\n",
       "      <td>19</td>\n",
       "      <td>1527921905</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6090</th>\n",
       "      <td>6090</td>\n",
       "      <td>1.1140</td>\n",
       "      <td>2.6262</td>\n",
       "      <td>11</td>\n",
       "      <td>4000153867</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6234</th>\n",
       "      <td>6234</td>\n",
       "      <td>1.1449</td>\n",
       "      <td>2.5003</td>\n",
       "      <td>34</td>\n",
       "      <td>3741484405</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6350</th>\n",
       "      <td>6350</td>\n",
       "      <td>1.0844</td>\n",
       "      <td>2.7436</td>\n",
       "      <td>65</td>\n",
       "      <td>5963693798</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7468</th>\n",
       "      <td>7468</td>\n",
       "      <td>1.0058</td>\n",
       "      <td>2.5096</td>\n",
       "      <td>66</td>\n",
       "      <td>9076695703</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8478</th>\n",
       "      <td>8478</td>\n",
       "      <td>1.2015</td>\n",
       "      <td>2.5187</td>\n",
       "      <td>72</td>\n",
       "      <td>3992589015</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:02:57.696047Z",
     "start_time": "2025-05-09T02:02:57.684047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "place_count = data.groupby('place_id').count()\n",
    "place_count"
   ],
   "id": "205ecd0427cc28c3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            row_id     x     y  accuracy   day  hour  weekday\n",
       "place_id                                                     \n",
       "1012023972       1     1     1         1     1     1        1\n",
       "1057182134       1     1     1         1     1     1        1\n",
       "1059958036       3     3     3         3     3     3        3\n",
       "1085266789       1     1     1         1     1     1        1\n",
       "1097200869    1044  1044  1044      1044  1044  1044     1044\n",
       "...            ...   ...   ...       ...   ...   ...      ...\n",
       "9904182060       1     1     1         1     1     1        1\n",
       "9915093501       1     1     1         1     1     1        1\n",
       "9946198589       1     1     1         1     1     1        1\n",
       "9950190890       1     1     1         1     1     1        1\n",
       "9980711012       5     5     5         5     5     5        5\n",
       "\n",
       "[805 rows x 7 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1012023972</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057182134</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059958036</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085266789</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097200869</th>\n",
       "      <td>1044</td>\n",
       "      <td>1044</td>\n",
       "      <td>1044</td>\n",
       "      <td>1044</td>\n",
       "      <td>1044</td>\n",
       "      <td>1044</td>\n",
       "      <td>1044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9904182060</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9915093501</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9946198589</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9950190890</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980711012</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>805 rows × 7 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:10:11.779678Z",
     "start_time": "2025-05-09T02:10:11.771787Z"
    }
   },
   "cell_type": "code",
   "source": "place_count['x'].describe() #打卡地点总计805个，50%打卡小于2次",
   "id": "e39a34f83d3ce0f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     805.000000\n",
       "mean       22.000000\n",
       "std        88.955632\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         5.000000\n",
       "max      1044.000000\n",
       "Name: x, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:12:35.926515Z",
     "start_time": "2025-05-09T02:12:35.920458Z"
    }
   },
   "cell_type": "code",
   "source": "place_count['x']",
   "id": "158bc943e1c05e30",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "place_id\n",
       "1012023972       1\n",
       "1057182134       1\n",
       "1059958036       3\n",
       "1085266789       1\n",
       "1097200869    1044\n",
       "              ... \n",
       "9904182060       1\n",
       "9915093501       1\n",
       "9946198589       1\n",
       "9950190890       1\n",
       "9980711012       5\n",
       "Name: x, Length: 805, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:35:35.206016Z",
     "start_time": "2025-05-09T02:35:35.195019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # 把index变为0,1,2，3,4,5,6这种效果，从零开始排，原来的index是row_id\n",
    "#只选择去的人大于3的数据，认为1,2,3的是噪音，这个地方去的人很少，不用推荐给其他人\n",
    "tf = place_count[place_count.row_id > 3].reset_index()\n",
    "tf  #剩余的签到地点"
   ],
   "id": "4a57320e945f1435",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       place_id  row_id     x     y  accuracy   day  hour  weekday\n",
       "0    1097200869    1044  1044  1044      1044  1044  1044     1044\n",
       "1    1228935308     120   120   120       120   120   120      120\n",
       "2    1267801529      58    58    58        58    58    58       58\n",
       "3    1278040507      15    15    15        15    15    15       15\n",
       "4    1285051622      21    21    21        21    21    21       21\n",
       "..          ...     ...   ...   ...       ...   ...   ...      ...\n",
       "234  9741307878       5     5     5         5     5     5        5\n",
       "235  9753855529      21    21    21        21    21    21       21\n",
       "236  9806043737       6     6     6         6     6     6        6\n",
       "237  9809476069      23    23    23        23    23    23       23\n",
       "238  9980711012       5     5     5         5     5     5        5\n",
       "\n",
       "[239 rows x 8 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>place_id</th>\n",
       "      <th>row_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1097200869</td>\n",
       "      <td>1044</td>\n",
       "      <td>1044</td>\n",
       "      <td>1044</td>\n",
       "      <td>1044</td>\n",
       "      <td>1044</td>\n",
       "      <td>1044</td>\n",
       "      <td>1044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1228935308</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1267801529</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1278040507</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1285051622</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>9741307878</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>9753855529</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>9806043737</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>9809476069</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>9980711012</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239 rows × 8 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:35:38.383502Z",
     "start_time": "2025-05-09T02:35:38.375373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = data[data['place_id'].isin(tf.place_id)]\n",
    "data.shape"
   ],
   "id": "fcf09272ba263d73",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16918, 8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:36:12.683713Z",
     "start_time": "2025-05-09T02:36:12.676892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # 取出数据当中的特征值和目标值\n",
    "y = data['place_id']\n",
    "# 删除目标值，保留特征值，\n",
    "x = data.drop(['place_id'], axis=1)\n",
    "# 删除无用的特征值，row_id是索引,这就是噪音\n",
    "x = x.drop(['row_id'], axis=1)\n",
    "print(x.shape)\n",
    "print(x.columns)"
   ],
   "id": "868b647c0890c8c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16918, 6)\n",
      "Index(['x', 'y', 'accuracy', 'day', 'hour', 'weekday'], dtype='object')\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 上面预处理完成",
   "id": "fe2cbf68c4b3572c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:48:19.730387Z",
     "start_time": "2025-05-09T02:48:19.718537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)\n",
    "std = StandardScaler()\n",
    "x_train = std.fit_transform(x_train)\n",
    "print(std.mean_)\n",
    "print(std.var_)\n",
    "x_test = std.transform(x_test)\n",
    "print('-' * 50)\n",
    "print(std.mean_)\n",
    "print(std.var_)"
   ],
   "id": "5c6c11331688759f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.12295735  2.63237278 81.34938525  5.10064628 11.44293821  3.10135561]\n",
      "[5.98489138e-03 4.86857391e-03 1.19597480e+04 7.32837915e+00\n",
      " 4.83742660e+01 2.81838404e+00]\n",
      "--------------------------------------------------\n",
      "[ 1.12295735  2.63237278 81.34938525  5.10064628 11.44293821  3.10135561]\n",
      "[5.98489138e-03 4.86857391e-03 1.19597480e+04 7.32837915e+00\n",
      " 4.83742660e+01 2.81838404e+00]\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:48:20.477822Z",
     "start_time": "2025-05-09T02:48:20.472456Z"
    }
   },
   "cell_type": "code",
   "source": "x_train.shape",
   "id": "2e46f3b04eb27c85",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12688, 6)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T02:48:21.358236Z",
     "start_time": "2025-05-09T02:48:20.999010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # 进行算法流程 # 超参数，可以通过设置n_neighbors=5，来调整结果好坏\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# # fit， predict,score，训练，knn的fit是不训练的，只是把训练集的特征值和目标值放入到内存中\n",
    "knn.fit(x_train, y_train)\n",
    "# # #\n",
    "# # # 得出预测结果\n",
    "y_predict = knn.predict(x_test)\n",
    "# #\n",
    "print(\"预测的目标签到位置为：\", y_predict[0:10])\n",
    "# # #\n",
    "# # # # 得出准确率,是评估指标\n",
    "print(\"预测的准确率:\", knn.score(x_test, y_test))\n",
    "# print(y_predict)\n",
    "# y_test"
   ],
   "id": "6dd9827e165cbed1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测的目标签到位置为： [5689129232 1097200869 2355236719 9632980559 6424972551 4022692381\n",
      " 8048985799 6683426742 1435128522 3312463746]\n",
      "预测的准确率: 0.484160756501182\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 调超参的方法，网格搜索",
   "id": "36edac0712465571"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:54:24.728579Z",
     "start_time": "2025-05-09T11:54:19.750145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#网格搜索时讲解\n",
    "# # 构造一些参数（超参）的值进行搜索\n",
    "# uniform是考虑距离\n",
    "# uniform\t直接投票：A(2票) vs B(1票)\t\n",
    "# distance\t加权投票：A(1/1 + 1/3=1.33) > B(1/2=0.5)\n",
    "param = {\"n_neighbors\": [3, 5, 10, 12, 15],'weights':['uniform', 'distance']}\n",
    "\n",
    "# 进行网格搜索，cv=3是3折交叉验证，用其中2折训练，1折验证\n",
    "gc = GridSearchCV(knn, param_grid=param, cv=3)\n",
    "\n",
    "gc.fit(x_train, y_train)  #你给它的x_train，它又分为训练集，验证集\n",
    "\n",
    "# 预测准确率，为了给大家看看\n",
    "print(\"在测试集上准确率：\", gc.score(x_test, y_test))\n",
    "\n",
    "print(\"在交叉验证当中最好的结果：\", gc.best_score_) #最好的结果\n",
    "\n",
    "print(\"选择最好的模型是：\", gc.best_estimator_) #最好的模型,告诉你用了哪些参数\n",
    "\n",
    "print(\"每个超参数每次交叉验证的结果：\")\n",
    "gc.cv_results_"
   ],
   "id": "47d3dcdcb5cef62b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在测试集上准确率： 0.49763593380614657\n",
      "在交叉验证当中最好的结果： 0.4816362349278435\n",
      "选择最好的模型是： KNeighborsClassifier(n_neighbors=12, weights='distance')\n",
      "每个超参数每次交叉验证的结果：\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01878587, 0.00900785, 0.01233641, 0.01218708, 0.01433436,\n",
       "        0.00999602, 0.00887696, 0.00933599, 0.00800125, 0.00800101]),\n",
       " 'std_fit_time': array([1.59688077e-02, 1.20274763e-05, 1.69714705e-03, 1.66782399e-03,\n",
       "        1.23441030e-03, 2.82833462e-03, 6.74615957e-04, 1.25089212e-03,\n",
       "        5.55514721e-06, 2.65015475e-06]),\n",
       " 'mean_score_time': array([0.16636395, 0.05923406, 0.23212663, 0.08556271, 0.27104282,\n",
       "        0.08147152, 0.17997289, 0.09253295, 0.18457015, 0.09795562]),\n",
       " 'std_score_time': array([0.01859669, 0.00660515, 0.01717608, 0.00891144, 0.0018389 ,\n",
       "        0.00558234, 0.00158238, 0.00222872, 0.00107326, 0.00123053]),\n",
       " 'param_n_neighbors': masked_array(data=[3, 3, 5, 5, 10, 10, 12, 12, 15, 15],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value=999999),\n",
       " 'param_weights': masked_array(data=['uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance', 'uniform', 'distance',\n",
       "                    'uniform', 'distance'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value=np.str_('?'),\n",
       "             dtype=object),\n",
       " 'params': [{'n_neighbors': 3, 'weights': 'uniform'},\n",
       "  {'n_neighbors': 3, 'weights': 'distance'},\n",
       "  {'n_neighbors': 5, 'weights': 'uniform'},\n",
       "  {'n_neighbors': 5, 'weights': 'distance'},\n",
       "  {'n_neighbors': 10, 'weights': 'uniform'},\n",
       "  {'n_neighbors': 10, 'weights': 'distance'},\n",
       "  {'n_neighbors': 12, 'weights': 'uniform'},\n",
       "  {'n_neighbors': 12, 'weights': 'distance'},\n",
       "  {'n_neighbors': 15, 'weights': 'uniform'},\n",
       "  {'n_neighbors': 15, 'weights': 'distance'}],\n",
       " 'split0_test_score': array([0.44468085, 0.4534279 , 0.4607565 , 0.47399527, 0.46170213,\n",
       "        0.48014184, 0.45650118, 0.48108747, 0.45508274, 0.47895981]),\n",
       " 'split1_test_score': array([0.43390873, 0.4542445 , 0.45660913, 0.47528967, 0.45542681,\n",
       "        0.48238354, 0.45329865, 0.48049184, 0.44809648, 0.47623552]),\n",
       " 'split2_test_score': array([0.43982029, 0.4561362 , 0.45684559, 0.47221565, 0.4618113 ,\n",
       "        0.48191062, 0.45897375, 0.48332939, 0.46062899, 0.48049184]),\n",
       " 'mean_test_score': array([0.43946996, 0.45460287, 0.45807041, 0.47383353, 0.45964675,\n",
       "        0.48147867, 0.45625786, 0.48163623, 0.45460274, 0.47856239]),\n",
       " 'std_test_score': array([0.00440467, 0.00113433, 0.00190181, 0.00126016, 0.00298428,\n",
       "        0.00096479, 0.00232323, 0.00122169, 0.00512762, 0.00176021]),\n",
       " 'rank_test_score': array([10,  8,  6,  4,  5,  2,  7,  1,  9,  3], dtype=int32)}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T11:59:41.244628Z",
     "start_time": "2025-05-09T11:59:41.235267Z"
    }
   },
   "cell_type": "code",
   "source": "(0.48108747 +  0.48049184 + 0.48332939) / 3",
   "id": "60bacd30365e0db7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48163623333333333"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:12:41.022383Z",
     "start_time": "2025-05-09T14:12:40.587188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "news = fetch_20newsgroups(subset='all', data_home='../python_ml/data')\n",
    "print(len(news.data))\n",
    "print('-'*50)\n",
    "print(news.data[0]) #第一个样本 特征\n",
    "print('-'*50)\n",
    "print(news.target) #标签\n",
    "print(np.unique(news.target)) #标签的类别\n",
    "print(news.target_names) #标签的名字\n"
   ],
   "id": "e33bbe280d54b5c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "--------------------------------------------------\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "[10  3 17 ...  3  1  7]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:15:20.448488Z",
     "start_time": "2025-05-09T14:15:13.581604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('-'*50)\n",
    "# 进行数据分割\n",
    "x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25, random_state=1)\n",
    "\n",
    "# 对数据集进行特征抽取\n",
    "tf = TfidfVectorizer()\n",
    "\n",
    "# 以训练集当中的词的列表进行每篇文章重要性统计['a','b','c','d']\n",
    "x_train = tf.fit_transform(x_train)\n",
    "#针对特征内容，可以自行打印，下面的打印可以得到特征数目，总计有15万特征\n",
    "print(len(tf.get_feature_names_out()))\n",
    "print('-' * 50)\n"
   ],
   "id": "14b288bd9dc7b35c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "153196\n",
      "--------------------------------------------------\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:16:31.114489Z",
     "start_time": "2025-05-09T14:16:30.944364Z"
    }
   },
   "cell_type": "code",
   "source": "print(tf.get_feature_names_out()[100000])",
   "id": "73157605cbcbaf9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "murky\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:17:47.464102Z",
     "start_time": "2025-05-09T14:17:47.292323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "start = time.time()\n",
    "mlt = MultinomialNB(alpha=1.0)\n",
    "mlt.fit(x_train, y_train)\n",
    "end = time.time()\n",
    "end - start"
   ],
   "id": "90b1afc56a34017c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16277027130126953"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:18:54.769349Z",
     "start_time": "2025-05-09T14:18:53.274933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_transform_test = tf.transform(x_test)  #特征数目不发生改变\n",
    "print(len(tf.get_feature_names_out())) #查看特征数目"
   ],
   "id": "effce19d7ac0d019",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153196\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:31:49.343064Z",
     "start_time": "2025-05-09T14:31:49.283056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start=time.time()\n",
    "y_predict = mlt.predict(x_transform_test)\n",
    "\n",
    "print(\"预测的前面10篇文章类别为：\", y_predict[0:10])\n",
    "\n",
    "# 得出准确率,这个是很难提高准确率，为什么呢？\n",
    "print(\"准确率为：\", mlt.score(x_transform_test, y_test))\n",
    "end=time.time()\n",
    "end-start"
   ],
   "id": "4cb915b9d7966b24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测的前面10篇文章类别为： [16 19 18  1  9 15  1  2 16 13]\n",
      "准确率为： 0.8518675721561969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.053552865982055664"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:32:07.655443Z",
     "start_time": "2025-05-09T14:32:07.648184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#预测的文章数目\n",
    "len(y_predict)"
   ],
   "id": "965e7ff099b9ab84",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4712"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:33:16.920620Z",
     "start_time": "2025-05-09T14:33:16.897615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 目前这个场景我们不需要召回率，support是真实的为那个类别的有多少个样本\n",
    "print(classification_report(y_test, y_predict,\n",
    "      target_names=news.target_names))"
   ],
   "id": "18a9d3f71301ab89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.91      0.77      0.83       199\n",
      "           comp.graphics       0.83      0.79      0.81       242\n",
      " comp.os.ms-windows.misc       0.89      0.83      0.86       263\n",
      "comp.sys.ibm.pc.hardware       0.80      0.83      0.81       262\n",
      "   comp.sys.mac.hardware       0.90      0.88      0.89       234\n",
      "          comp.windows.x       0.92      0.85      0.88       230\n",
      "            misc.forsale       0.96      0.67      0.79       257\n",
      "               rec.autos       0.90      0.87      0.88       265\n",
      "         rec.motorcycles       0.90      0.95      0.92       251\n",
      "      rec.sport.baseball       0.89      0.96      0.93       226\n",
      "        rec.sport.hockey       0.95      0.98      0.96       262\n",
      "               sci.crypt       0.76      0.97      0.85       257\n",
      "         sci.electronics       0.84      0.80      0.82       229\n",
      "                 sci.med       0.97      0.86      0.91       249\n",
      "               sci.space       0.92      0.96      0.94       256\n",
      "  soc.religion.christian       0.55      0.98      0.70       243\n",
      "      talk.politics.guns       0.76      0.96      0.85       234\n",
      "   talk.politics.mideast       0.93      0.99      0.96       224\n",
      "      talk.politics.misc       0.98      0.56      0.72       197\n",
      "      talk.religion.misc       0.97      0.26      0.41       132\n",
      "\n",
      "                accuracy                           0.85      4712\n",
      "               macro avg       0.88      0.84      0.84      4712\n",
      "            weighted avg       0.87      0.85      0.85      4712\n",
      "\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:37:25.416255Z",
     "start_time": "2025-05-09T14:37:25.410255Z"
    }
   },
   "cell_type": "code",
   "source": "y_test.shape #测试集中有多少 样本",
   "id": "f49a2d7f777aad65",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4712,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:38:25.513172Z",
     "start_time": "2025-05-09T14:38:25.507955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_test1 = np.where(y_test == 0, 1, 0)\n",
    "print(y_test1.sum())"
   ],
   "id": "6b3585cfe8847577",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:39:30.280196Z",
     "start_time": "2025-05-09T14:39:30.275220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_predict1 = np.where(y_predict == 0, 1, 0)\n",
    "print(y_predict1.sum())"
   ],
   "id": "4dba1824087dbe8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:39:46.109028Z",
     "start_time": "2025-05-09T14:39:46.103026Z"
    }
   },
   "cell_type": "code",
   "source": "(y_test1*y_predict1).sum()",
   "id": "3f75ef7cc26f7a30",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(153)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:40:35.522717Z",
     "start_time": "2025-05-09T14:40:35.516739Z"
    }
   },
   "cell_type": "code",
   "source": "153/168 # 精确率",
   "id": "8bcf0915536bb546",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9107142857142857"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T14:40:51.867408Z",
     "start_time": "2025-05-09T14:40:51.863141Z"
    }
   },
   "cell_type": "code",
   "source": "153/199 # 召回率",
   "id": "26a3b88274510e0c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7688442211055276"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T15:01:14.154811Z",
     "start_time": "2025-05-09T15:01:14.104276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 把0-19总计20个分类，变为0和1\n",
    "# 5是可以改为0到19的\n",
    "y_test1 = np.where(y_test == 5, 1, 0)\n",
    "print(y_test1.sum()) #label为5的样本数\n",
    "y_predict1 = np.where(y_predict == 5, 1, 0)\n",
    "print(y_predict1.sum())\n",
    "# roc_auc_score的y_test只能是二分类,针对多分类如何计算AUC\n",
    "print(\"AUC指标：\", roc_auc_score(y_test1, y_predict1))"
   ],
   "id": "7c64917fb98d3958",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n",
      "214\n",
      "AUC指标： 0.924078924393225\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4652b54cec025a6a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
